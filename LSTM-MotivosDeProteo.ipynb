{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288796b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174a6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/MotivosDeProteo.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc5ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motivos de Proteo\n",
      "José Enrique Rodó\n",
      "\n",
      "\n",
      "\n",
      "     No publico una «primera parte» de PROTEO: el material que he apartado para estos «Motivos» da, en compendio, idea general de la obra, harto extensa (aun si la limitase a lo que tengo escrito) para ser editada de una vez. Los claros de este volumen serán el contenido del siguiente y así en los sucesivos. Y nunca PROTEO se publicará de otro modo que de éste; es decir: nunca le daré «arquitectura» concreta, ni término forzoso: siempre podrá seguir desenvolviéndose, «viviendo». La índole del libro (si tal puede llamársele) consiente, en torno de un pensamiento capital, tan vasta ramificación de ideas y motivos, que nada se opone a que haga de él lo que quiero que sea: un libro en perpetuo «devenir», un libro abierto sobre una perspectiva indefinida.\n",
      "\n",
      "J. E. R.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "- I -\n",
      "Reformarse es vivir. Nuestra transformación personal en el tiempo.\n",
      "\n",
      "\n",
      "Reformarse es vivir... Y desde luego, nuestra transformación personal en cierto grado ¿no es ley constante e \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c85485e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672786"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b301d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6c557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num --> letra\n",
    "\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd3ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letra --> num\n",
    "\n",
    "encoder = {char:ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fba4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b33e0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 77, 21, 40, 42, 77, 80, 50, 55, 39, 50, 53, 15, 77, 21, 39, 77,\n",
       "       57, 67, 77, 80, 51, 50, 23, 49, 15, 40, 71, 14, 39, 50,  8, 77, 55,\n",
       "       56, 57, 57, 57, 57, 50, 50, 50, 50, 50, 16, 77, 50, 27, 14, 74, 79,\n",
       "       40, 28, 77, 50, 14, 49, 38, 50, 35, 27, 15, 40,  0, 39, 15, 38, 50,\n",
       "       27, 38, 15, 21, 39, 24, 50, 55, 39, 50, 53,  8, 62, 86, 23, 62, 52,\n",
       "       50, 39, 79, 50,  0, 38, 21, 39, 15, 40, 38, 79, 50, 71, 14, 39, 50,\n",
       "       19, 39, 50, 38, 27, 38, 15, 21, 38, 55, 77, 50, 27, 38, 15, 38, 50,\n",
       "       39, 80, 21, 77, 80, 50, 35, 17, 77, 21, 40, 42, 77, 80, 24, 50, 55,\n",
       "       38,  9, 50, 39, 49, 50, 28, 77,  0, 27, 39, 49, 55, 40, 77,  9, 50,\n",
       "       40, 55, 39, 38, 50, 25, 39, 49, 39, 15, 38, 79, 50, 55, 39, 50, 79,\n",
       "       38, 50, 77, 74, 15, 38,  9, 50, 19, 38, 15, 21, 77, 50, 39, 18, 21,\n",
       "       39, 49, 80, 38, 50, 44, 38, 14, 49, 50, 80, 40, 50, 79, 38, 50, 79,\n",
       "       40,  0, 40, 21, 38, 80, 39, 50, 38, 50, 79, 77, 50, 71, 14, 39, 50,\n",
       "       21, 39, 49, 25, 77, 50, 39, 80, 28, 15, 40, 21, 77, 41, 50, 27, 38,\n",
       "       15, 38, 50, 80, 39, 15, 50, 39, 55, 40, 21, 38, 55, 38, 50, 55, 39,\n",
       "       50, 14, 49, 38, 50, 42, 39, 10, 34, 50, 70, 77, 80, 50, 28, 79, 38,\n",
       "       15, 77, 80, 50, 55, 39, 50, 39, 80, 21, 39, 50, 42, 77, 79, 14,  0,\n",
       "       39, 49, 50, 80, 39, 15, 68, 49, 50, 39, 79, 50, 28, 77, 49, 21, 39,\n",
       "       49, 40, 55, 77, 50, 55, 39, 79, 50, 80, 40, 25, 14, 40, 39, 49, 21,\n",
       "       39, 50,  3, 50, 38, 80, 84, 50, 39, 49, 50, 79, 77, 80, 50, 80, 14,\n",
       "       28, 39, 80, 40, 42, 77, 80, 34, 50, 58, 50, 49, 14, 49, 28, 38, 50,\n",
       "       53,  8, 62, 86, 23, 62, 50, 80, 39, 50, 27, 14, 74, 79, 40, 28, 38,\n",
       "       15, 68, 50, 55, 39, 50, 77, 21, 15, 77, 50,  0, 77, 55, 77, 50, 71,\n",
       "       14, 39, 50, 55, 39, 50, 51, 80, 21, 39, 82, 50, 39, 80, 50, 55, 39,\n",
       "       28, 40, 15, 52, 50, 49, 14, 49, 28, 38, 50, 79, 39, 50, 55, 38, 15,\n",
       "       51, 50, 35, 38, 15, 71, 14, 40, 21, 39, 28, 21, 14, 15, 38, 24, 50,\n",
       "       28, 77, 49, 28, 15, 39, 21, 38,  9, 50, 49, 40, 50, 21, 51, 15,  0,\n",
       "       40, 49, 77, 50,  4, 77, 15, 10, 77, 80, 77, 52, 50, 80, 40, 39,  0,\n",
       "       27, 15, 39, 50, 27, 77, 55, 15, 68, 50, 80, 39, 25, 14, 40, 15, 50,\n",
       "       55, 39, 80, 39, 49, 42, 77])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82f2cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_txt, num_unique_chars):\n",
    "    #encoded_txt: batch encoded text\n",
    "    #num_unique_chars: len of all characters\n",
    "    \n",
    "    one_hot = np.zeros((encoded_txt.size, num_unique_chars))\n",
    "    \n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]),encoded_txt.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*encoded_txt.shape, num_unique_chars))\n",
    "    \n",
    "    return one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b21041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_txt, samp_per_batch=10, seq_len=50):\n",
    "    #X : encoded text of length seq_len\n",
    "    #Y : encoded text shifted by one\n",
    "    \n",
    "    #how many chars per batch\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # how many batches can we make with the given text\n",
    "    num_batches_avail = int(len(encoded_txt)/char_per_batch)\n",
    "    \n",
    "    #cut off the end of the encoded text that we can't fit evenly into a batch\n",
    "    encoded_txt = encoded_txt[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    encoded_txt = encoded_txt.reshape((samp_per_batch),-1)\n",
    "    \n",
    "    for n in range(0, encoded_txt.shape[1], seq_len):\n",
    "        \n",
    "        x = encoded_txt[:,n:n+seq_len]\n",
    "        \n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_txt[:,n+seq_len]\n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_txt[:,0]\n",
    "            \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b9b53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f09d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a7d4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ddcf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 77, 21, 40, 42, 77],\n",
       "       [50, 51, 80, 21, 39, 82]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e26a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 21, 40, 42, 77, 80],\n",
       "       [51, 80, 21, 39, 82, 50]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b09cdc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers = 4, drop_prob = 0.5, use_gpu=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        self.all_chars = all_chars\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        #encoder\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1,self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out,hidden\n",
    "    \n",
    "    def hidden_state(self,batch_size):\n",
    "        \n",
    "        if self.use_gpu:          \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39a502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_chars=all_characters,\n",
    "                 num_hidden=256,\n",
    "                 num_layers=4,\n",
    "                 drop_prob=0.4,\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f694e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03bddcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958490"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91138d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a851147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48ecd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28175bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "test_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19d0dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "epochs = 100\n",
    "batch_size = 80\n",
    "\n",
    "seq_len = 150\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1ecad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 15  TEST LOSS: 3.01088809967041\n",
      "Epoch: 0  Step: 30  TEST LOSS: 2.981618881225586\n",
      "Epoch: 1  Step: 45  TEST LOSS: 2.980289936065674\n",
      "Epoch: 1  Step: 60  TEST LOSS: 2.9801018238067627\n",
      "Epoch: 1  Step: 75  TEST LOSS: 2.9791440963745117\n",
      "Epoch: 2  Step: 90  TEST LOSS: 2.9783995151519775\n",
      "Epoch: 2  Step: 105  TEST LOSS: 2.9780454635620117\n",
      "Epoch: 3  Step: 120  TEST LOSS: 2.977144956588745\n",
      "Epoch: 3  Step: 135  TEST LOSS: 2.978137969970703\n",
      "Epoch: 3  Step: 150  TEST LOSS: 2.977324962615967\n",
      "Epoch: 4  Step: 165  TEST LOSS: 2.977097749710083\n",
      "Epoch: 4  Step: 180  TEST LOSS: 2.977449655532837\n",
      "Epoch: 4  Step: 195  TEST LOSS: 2.977058172225952\n",
      "Epoch: 5  Step: 210  TEST LOSS: 2.977452039718628\n",
      "Epoch: 5  Step: 225  TEST LOSS: 2.9769606590270996\n",
      "Epoch: 6  Step: 240  TEST LOSS: 2.9763436317443848\n",
      "Epoch: 6  Step: 255  TEST LOSS: 2.9777090549468994\n",
      "Epoch: 6  Step: 270  TEST LOSS: 2.976966142654419\n",
      "Epoch: 7  Step: 285  TEST LOSS: 2.976698160171509\n",
      "Epoch: 7  Step: 300  TEST LOSS: 2.976670265197754\n",
      "Epoch: 8  Step: 315  TEST LOSS: 2.9761126041412354\n",
      "Epoch: 8  Step: 330  TEST LOSS: 2.977184295654297\n",
      "Epoch: 8  Step: 345  TEST LOSS: 2.976564884185791\n",
      "Epoch: 9  Step: 360  TEST LOSS: 2.976412773132324\n",
      "Epoch: 9  Step: 375  TEST LOSS: 2.9769628047943115\n",
      "Epoch: 9  Step: 390  TEST LOSS: 2.9765801429748535\n",
      "Epoch: 10  Step: 405  TEST LOSS: 2.976877212524414\n",
      "Epoch: 10  Step: 420  TEST LOSS: 2.9763853549957275\n",
      "Epoch: 11  Step: 435  TEST LOSS: 2.9751803874969482\n",
      "Epoch: 11  Step: 450  TEST LOSS: 2.972788095474243\n",
      "Epoch: 11  Step: 465  TEST LOSS: 2.9375555515289307\n",
      "Epoch: 12  Step: 480  TEST LOSS: 2.8682548999786377\n",
      "Epoch: 12  Step: 495  TEST LOSS: 2.7789390087127686\n",
      "Epoch: 13  Step: 510  TEST LOSS: 2.743398427963257\n",
      "Epoch: 13  Step: 525  TEST LOSS: 2.706835985183716\n",
      "Epoch: 13  Step: 540  TEST LOSS: 2.6738529205322266\n",
      "Epoch: 14  Step: 555  TEST LOSS: 2.639026641845703\n",
      "Epoch: 14  Step: 570  TEST LOSS: 2.609323501586914\n",
      "Epoch: 14  Step: 585  TEST LOSS: 2.584352493286133\n",
      "Epoch: 15  Step: 600  TEST LOSS: 2.5494935512542725\n",
      "Epoch: 15  Step: 615  TEST LOSS: 2.5153696537017822\n",
      "Epoch: 16  Step: 630  TEST LOSS: 2.4711875915527344\n",
      "Epoch: 16  Step: 645  TEST LOSS: 2.397250175476074\n",
      "Epoch: 16  Step: 660  TEST LOSS: 2.326510190963745\n",
      "Epoch: 17  Step: 675  TEST LOSS: 2.284428834915161\n",
      "Epoch: 17  Step: 690  TEST LOSS: 2.2647101879119873\n",
      "Epoch: 18  Step: 705  TEST LOSS: 2.242069959640503\n",
      "Epoch: 18  Step: 720  TEST LOSS: 2.2282824516296387\n",
      "Epoch: 18  Step: 735  TEST LOSS: 2.2153265476226807\n",
      "Epoch: 19  Step: 750  TEST LOSS: 2.2002062797546387\n",
      "Epoch: 19  Step: 765  TEST LOSS: 2.1889283657073975\n",
      "Epoch: 19  Step: 780  TEST LOSS: 2.181774377822876\n",
      "Epoch: 20  Step: 795  TEST LOSS: 2.174882411956787\n",
      "Epoch: 20  Step: 810  TEST LOSS: 2.160003900527954\n",
      "Epoch: 21  Step: 825  TEST LOSS: 2.150947332382202\n",
      "Epoch: 21  Step: 840  TEST LOSS: 2.1400766372680664\n",
      "Epoch: 21  Step: 855  TEST LOSS: 2.1378090381622314\n",
      "Epoch: 22  Step: 870  TEST LOSS: 2.1205718517303467\n",
      "Epoch: 22  Step: 885  TEST LOSS: 2.1173222064971924\n",
      "Epoch: 23  Step: 900  TEST LOSS: 2.1072394847869873\n",
      "Epoch: 23  Step: 915  TEST LOSS: 2.0974678993225098\n",
      "Epoch: 23  Step: 930  TEST LOSS: 2.0947041511535645\n",
      "Epoch: 24  Step: 945  TEST LOSS: 2.0825905799865723\n",
      "Epoch: 24  Step: 960  TEST LOSS: 2.076774835586548\n",
      "Epoch: 24  Step: 975  TEST LOSS: 2.068372964859009\n",
      "Epoch: 25  Step: 990  TEST LOSS: 2.0641942024230957\n",
      "Epoch: 25  Step: 1005  TEST LOSS: 2.0564563274383545\n",
      "Epoch: 26  Step: 1020  TEST LOSS: 2.0484771728515625\n",
      "Epoch: 26  Step: 1035  TEST LOSS: 2.0473783016204834\n",
      "Epoch: 26  Step: 1050  TEST LOSS: 2.038426399230957\n",
      "Epoch: 27  Step: 1065  TEST LOSS: 2.02803373336792\n",
      "Epoch: 27  Step: 1080  TEST LOSS: 2.0250887870788574\n",
      "Epoch: 28  Step: 1095  TEST LOSS: 2.018325090408325\n",
      "Epoch: 28  Step: 1110  TEST LOSS: 2.0120837688446045\n",
      "Epoch: 28  Step: 1125  TEST LOSS: 2.0110554695129395\n",
      "Epoch: 29  Step: 1140  TEST LOSS: 1.9998116493225098\n",
      "Epoch: 29  Step: 1155  TEST LOSS: 1.9967148303985596\n",
      "Epoch: 29  Step: 1170  TEST LOSS: 1.9938322305679321\n",
      "Epoch: 30  Step: 1185  TEST LOSS: 1.9887768030166626\n",
      "Epoch: 30  Step: 1200  TEST LOSS: 1.9820867776870728\n",
      "Epoch: 31  Step: 1215  TEST LOSS: 1.9792921543121338\n",
      "Epoch: 31  Step: 1230  TEST LOSS: 1.9720182418823242\n",
      "Epoch: 31  Step: 1245  TEST LOSS: 1.9670062065124512\n",
      "Epoch: 32  Step: 1260  TEST LOSS: 1.9620451927185059\n",
      "Epoch: 32  Step: 1275  TEST LOSS: 1.958938479423523\n",
      "Epoch: 33  Step: 1290  TEST LOSS: 1.9521100521087646\n",
      "Epoch: 33  Step: 1305  TEST LOSS: 1.9513978958129883\n",
      "Epoch: 33  Step: 1320  TEST LOSS: 1.946259617805481\n",
      "Epoch: 34  Step: 1335  TEST LOSS: 1.9387907981872559\n",
      "Epoch: 34  Step: 1350  TEST LOSS: 1.9378910064697266\n",
      "Epoch: 34  Step: 1365  TEST LOSS: 1.9337626695632935\n",
      "Epoch: 35  Step: 1380  TEST LOSS: 1.9301453828811646\n",
      "Epoch: 35  Step: 1395  TEST LOSS: 1.9237622022628784\n",
      "Epoch: 36  Step: 1410  TEST LOSS: 1.920213222503662\n",
      "Epoch: 36  Step: 1425  TEST LOSS: 1.9194624423980713\n",
      "Epoch: 36  Step: 1440  TEST LOSS: 1.9103163480758667\n",
      "Epoch: 37  Step: 1455  TEST LOSS: 1.9041569232940674\n",
      "Epoch: 37  Step: 1470  TEST LOSS: 1.905421257019043\n",
      "Epoch: 38  Step: 1485  TEST LOSS: 1.8979203701019287\n",
      "Epoch: 38  Step: 1500  TEST LOSS: 1.8950988054275513\n",
      "Epoch: 38  Step: 1515  TEST LOSS: 1.8961552381515503\n",
      "Epoch: 39  Step: 1530  TEST LOSS: 1.883440375328064\n",
      "Epoch: 39  Step: 1545  TEST LOSS: 1.8834245204925537\n",
      "Epoch: 39  Step: 1560  TEST LOSS: 1.8793286085128784\n",
      "Epoch: 40  Step: 1575  TEST LOSS: 1.8791322708129883\n",
      "Epoch: 40  Step: 1590  TEST LOSS: 1.8722519874572754\n",
      "Epoch: 41  Step: 1605  TEST LOSS: 1.8635807037353516\n",
      "Epoch: 41  Step: 1620  TEST LOSS: 1.8632231950759888\n",
      "Epoch: 41  Step: 1635  TEST LOSS: 1.859383463859558\n",
      "Epoch: 42  Step: 1650  TEST LOSS: 1.855265498161316\n",
      "Epoch: 42  Step: 1665  TEST LOSS: 1.8589545488357544\n",
      "Epoch: 43  Step: 1680  TEST LOSS: 1.8464256525039673\n",
      "Epoch: 43  Step: 1695  TEST LOSS: 1.8418214321136475\n",
      "Epoch: 43  Step: 1710  TEST LOSS: 1.8394564390182495\n",
      "Epoch: 44  Step: 1725  TEST LOSS: 1.8315764665603638\n",
      "Epoch: 44  Step: 1740  TEST LOSS: 1.8308095932006836\n",
      "Epoch: 44  Step: 1755  TEST LOSS: 1.8285211324691772\n",
      "Epoch: 45  Step: 1770  TEST LOSS: 1.8247231245040894\n",
      "Epoch: 45  Step: 1785  TEST LOSS: 1.8213990926742554\n",
      "Epoch: 46  Step: 1800  TEST LOSS: 1.8143757581710815\n",
      "Epoch: 46  Step: 1815  TEST LOSS: 1.8149114847183228\n",
      "Epoch: 46  Step: 1830  TEST LOSS: 1.8087118864059448\n",
      "Epoch: 47  Step: 1845  TEST LOSS: 1.8075573444366455\n",
      "Epoch: 47  Step: 1860  TEST LOSS: 1.8009965419769287\n",
      "Epoch: 48  Step: 1875  TEST LOSS: 1.7951246500015259\n",
      "Epoch: 48  Step: 1890  TEST LOSS: 1.791096568107605\n",
      "Epoch: 48  Step: 1905  TEST LOSS: 1.7923113107681274\n",
      "Epoch: 49  Step: 1920  TEST LOSS: 1.7815003395080566\n",
      "Epoch: 49  Step: 1935  TEST LOSS: 1.7826519012451172\n",
      "Epoch: 49  Step: 1950  TEST LOSS: 1.77783203125\n",
      "Epoch: 50  Step: 1965  TEST LOSS: 1.7746176719665527\n",
      "Epoch: 50  Step: 1980  TEST LOSS: 1.7710535526275635\n",
      "Epoch: 51  Step: 1995  TEST LOSS: 1.7656077146530151\n",
      "Epoch: 51  Step: 2010  TEST LOSS: 1.7666114568710327\n",
      "Epoch: 51  Step: 2025  TEST LOSS: 1.7622984647750854\n",
      "Epoch: 52  Step: 2040  TEST LOSS: 1.7595492601394653\n",
      "Epoch: 52  Step: 2055  TEST LOSS: 1.753349781036377\n",
      "Epoch: 53  Step: 2070  TEST LOSS: 1.7462680339813232\n",
      "Epoch: 53  Step: 2085  TEST LOSS: 1.745801568031311\n",
      "Epoch: 53  Step: 2100  TEST LOSS: 1.7476592063903809\n",
      "Epoch: 54  Step: 2115  TEST LOSS: 1.7373088598251343\n",
      "Epoch: 54  Step: 2130  TEST LOSS: 1.7376927137374878\n",
      "Epoch: 54  Step: 2145  TEST LOSS: 1.7333543300628662\n",
      "Epoch: 55  Step: 2160  TEST LOSS: 1.727536916732788\n",
      "Epoch: 55  Step: 2175  TEST LOSS: 1.7285468578338623\n",
      "Epoch: 56  Step: 2190  TEST LOSS: 1.7179887294769287\n",
      "Epoch: 56  Step: 2205  TEST LOSS: 1.722410798072815\n",
      "Epoch: 56  Step: 2220  TEST LOSS: 1.7159165143966675\n",
      "Epoch: 57  Step: 2235  TEST LOSS: 1.7101554870605469\n",
      "Epoch: 57  Step: 2250  TEST LOSS: 1.7067545652389526\n",
      "Epoch: 58  Step: 2265  TEST LOSS: 1.7048505544662476\n",
      "Epoch: 58  Step: 2280  TEST LOSS: 1.704211950302124\n",
      "Epoch: 58  Step: 2295  TEST LOSS: 1.7005090713500977\n",
      "Epoch: 59  Step: 2310  TEST LOSS: 1.6903512477874756\n",
      "Epoch: 59  Step: 2325  TEST LOSS: 1.6913896799087524\n",
      "Epoch: 59  Step: 2340  TEST LOSS: 1.6885807514190674\n",
      "Epoch: 60  Step: 2355  TEST LOSS: 1.6844472885131836\n",
      "Epoch: 60  Step: 2370  TEST LOSS: 1.6846855878829956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61  Step: 2385  TEST LOSS: 1.6730328798294067\n",
      "Epoch: 61  Step: 2400  TEST LOSS: 1.6769171953201294\n",
      "Epoch: 61  Step: 2415  TEST LOSS: 1.6733506917953491\n",
      "Epoch: 62  Step: 2430  TEST LOSS: 1.666959524154663\n",
      "Epoch: 62  Step: 2445  TEST LOSS: 1.6660690307617188\n",
      "Epoch: 63  Step: 2460  TEST LOSS: 1.6594417095184326\n",
      "Epoch: 63  Step: 2475  TEST LOSS: 1.6610177755355835\n",
      "Epoch: 63  Step: 2490  TEST LOSS: 1.6622159481048584\n",
      "Epoch: 64  Step: 2505  TEST LOSS: 1.6516740322113037\n",
      "Epoch: 64  Step: 2520  TEST LOSS: 1.653764009475708\n",
      "Epoch: 64  Step: 2535  TEST LOSS: 1.6480686664581299\n",
      "Epoch: 65  Step: 2550  TEST LOSS: 1.644834280014038\n",
      "Epoch: 65  Step: 2565  TEST LOSS: 1.6443257331848145\n",
      "Epoch: 66  Step: 2580  TEST LOSS: 1.63465416431427\n",
      "Epoch: 66  Step: 2595  TEST LOSS: 1.642552137374878\n",
      "Epoch: 66  Step: 2610  TEST LOSS: 1.6333867311477661\n",
      "Epoch: 67  Step: 2625  TEST LOSS: 1.6316252946853638\n",
      "Epoch: 67  Step: 2640  TEST LOSS: 1.6282155513763428\n",
      "Epoch: 68  Step: 2655  TEST LOSS: 1.6240662336349487\n",
      "Epoch: 68  Step: 2670  TEST LOSS: 1.623678207397461\n",
      "Epoch: 68  Step: 2685  TEST LOSS: 1.6251991987228394\n",
      "Epoch: 69  Step: 2700  TEST LOSS: 1.6139638423919678\n",
      "Epoch: 69  Step: 2715  TEST LOSS: 1.6138479709625244\n",
      "Epoch: 69  Step: 2730  TEST LOSS: 1.6124448776245117\n",
      "Epoch: 70  Step: 2745  TEST LOSS: 1.6081562042236328\n",
      "Epoch: 70  Step: 2760  TEST LOSS: 1.6086755990982056\n",
      "Epoch: 71  Step: 2775  TEST LOSS: 1.599812626838684\n",
      "Epoch: 71  Step: 2790  TEST LOSS: 1.6045820713043213\n",
      "Epoch: 71  Step: 2805  TEST LOSS: 1.600186824798584\n",
      "Epoch: 72  Step: 2820  TEST LOSS: 1.598834753036499\n",
      "Epoch: 72  Step: 2835  TEST LOSS: 1.5958839654922485\n",
      "Epoch: 73  Step: 2850  TEST LOSS: 1.5906912088394165\n",
      "Epoch: 73  Step: 2865  TEST LOSS: 1.5880696773529053\n",
      "Epoch: 73  Step: 2880  TEST LOSS: 1.5919981002807617\n",
      "Epoch: 74  Step: 2895  TEST LOSS: 1.5795994997024536\n",
      "Epoch: 74  Step: 2910  TEST LOSS: 1.5796606540679932\n",
      "Epoch: 74  Step: 2925  TEST LOSS: 1.5768136978149414\n",
      "Epoch: 75  Step: 2940  TEST LOSS: 1.5727285146713257\n",
      "Epoch: 75  Step: 2955  TEST LOSS: 1.5754804611206055\n",
      "Epoch: 76  Step: 2970  TEST LOSS: 1.5670552253723145\n",
      "Epoch: 76  Step: 2985  TEST LOSS: 1.5730963945388794\n",
      "Epoch: 76  Step: 3000  TEST LOSS: 1.5699783563613892\n",
      "Epoch: 77  Step: 3015  TEST LOSS: 1.5686413049697876\n",
      "Epoch: 77  Step: 3030  TEST LOSS: 1.5625537633895874\n",
      "Epoch: 78  Step: 3045  TEST LOSS: 1.5642385482788086\n",
      "Epoch: 78  Step: 3060  TEST LOSS: 1.5575315952301025\n",
      "Epoch: 78  Step: 3075  TEST LOSS: 1.557474136352539\n",
      "Epoch: 79  Step: 3090  TEST LOSS: 1.5475789308547974\n",
      "Epoch: 79  Step: 3105  TEST LOSS: 1.5547868013381958\n",
      "Epoch: 79  Step: 3120  TEST LOSS: 1.546951174736023\n",
      "Epoch: 80  Step: 3135  TEST LOSS: 1.5439105033874512\n",
      "Epoch: 80  Step: 3150  TEST LOSS: 1.5428951978683472\n",
      "Epoch: 81  Step: 3165  TEST LOSS: 1.5390528440475464\n",
      "Epoch: 81  Step: 3180  TEST LOSS: 1.539699912071228\n",
      "Epoch: 81  Step: 3195  TEST LOSS: 1.5363352298736572\n",
      "Epoch: 82  Step: 3210  TEST LOSS: 1.534499168395996\n",
      "Epoch: 82  Step: 3225  TEST LOSS: 1.5324591398239136\n",
      "Epoch: 83  Step: 3240  TEST LOSS: 1.5296064615249634\n",
      "Epoch: 83  Step: 3255  TEST LOSS: 1.5267395973205566\n",
      "Epoch: 83  Step: 3270  TEST LOSS: 1.5269407033920288\n",
      "Epoch: 84  Step: 3285  TEST LOSS: 1.521693229675293\n",
      "Epoch: 84  Step: 3300  TEST LOSS: 1.5217030048370361\n",
      "Epoch: 84  Step: 3315  TEST LOSS: 1.5153741836547852\n",
      "Epoch: 85  Step: 3330  TEST LOSS: 1.512260913848877\n",
      "Epoch: 85  Step: 3345  TEST LOSS: 1.5141801834106445\n",
      "Epoch: 86  Step: 3360  TEST LOSS: 1.5057474374771118\n",
      "Epoch: 86  Step: 3375  TEST LOSS: 1.5083948373794556\n",
      "Epoch: 86  Step: 3390  TEST LOSS: 1.5090945959091187\n",
      "Epoch: 87  Step: 3405  TEST LOSS: 1.500847339630127\n",
      "Epoch: 87  Step: 3420  TEST LOSS: 1.4998393058776855\n",
      "Epoch: 88  Step: 3435  TEST LOSS: 1.500606894493103\n",
      "Epoch: 88  Step: 3450  TEST LOSS: 1.4953033924102783\n",
      "Epoch: 88  Step: 3465  TEST LOSS: 1.49713933467865\n",
      "Epoch: 89  Step: 3480  TEST LOSS: 1.4876339435577393\n",
      "Epoch: 89  Step: 3495  TEST LOSS: 1.4914149045944214\n",
      "Epoch: 89  Step: 3510  TEST LOSS: 1.4900224208831787\n",
      "Epoch: 90  Step: 3525  TEST LOSS: 1.4857882261276245\n",
      "Epoch: 90  Step: 3540  TEST LOSS: 1.4870539903640747\n",
      "Epoch: 91  Step: 3555  TEST LOSS: 1.4798017740249634\n",
      "Epoch: 91  Step: 3570  TEST LOSS: 1.4808210134506226\n",
      "Epoch: 91  Step: 3585  TEST LOSS: 1.4851479530334473\n",
      "Epoch: 92  Step: 3600  TEST LOSS: 1.4775866270065308\n",
      "Epoch: 92  Step: 3615  TEST LOSS: 1.4742310047149658\n",
      "Epoch: 93  Step: 3630  TEST LOSS: 1.4764324426651\n",
      "Epoch: 93  Step: 3645  TEST LOSS: 1.472146987915039\n",
      "Epoch: 93  Step: 3660  TEST LOSS: 1.4696685075759888\n",
      "Epoch: 94  Step: 3675  TEST LOSS: 1.4629874229431152\n",
      "Epoch: 94  Step: 3690  TEST LOSS: 1.469126582145691\n",
      "Epoch: 94  Step: 3705  TEST LOSS: 1.464404821395874\n",
      "Epoch: 95  Step: 3720  TEST LOSS: 1.4641194343566895\n",
      "Epoch: 95  Step: 3735  TEST LOSS: 1.4635426998138428\n",
      "Epoch: 96  Step: 3750  TEST LOSS: 1.4580470323562622\n",
      "Epoch: 96  Step: 3765  TEST LOSS: 1.4577343463897705\n",
      "Epoch: 96  Step: 3780  TEST LOSS: 1.456500768661499\n",
      "Epoch: 97  Step: 3795  TEST LOSS: 1.4544637203216553\n",
      "Epoch: 97  Step: 3810  TEST LOSS: 1.4546562433242798\n",
      "Epoch: 98  Step: 3825  TEST LOSS: 1.4494024515151978\n",
      "Epoch: 98  Step: 3840  TEST LOSS: 1.4484176635742188\n",
      "Epoch: 98  Step: 3855  TEST LOSS: 1.4484385251998901\n",
      "Epoch: 99  Step: 3870  TEST LOSS: 1.440300464630127\n",
      "Epoch: 99  Step: 3885  TEST LOSS: 1.4425681829452515\n",
      "Epoch: 99  Step: 3900  TEST LOSS: 1.4391177892684937\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "        \n",
    "        tracker+=1\n",
    "        \n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output,hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if tracker % 15 == 0:\n",
    "            \n",
    "            test_hidden = model.hidden_state(batch_size)\n",
    "            test_losses = []\n",
    "            model.eval()\n",
    "            for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                \n",
    "                test_hidden = tuple([state.data for state in test_hidden])\n",
    "                \n",
    "                lstm_output,test_hidden = model.forward(inputs,test_hidden)\n",
    "                test_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "                \n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            \n",
    "            print(f'Epoch: {i}  Step: {tracker}  TEST LOSS: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5208bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hidden256_layers4_rodo.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8770fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d03ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model,char,hidden=None,k=1):\n",
    "    \n",
    "    encoded_text = model.encoder[char]\n",
    "    \n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    \n",
    "    encoded_text = one_hot_encoder(encoded_text,len(model.all_chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    lstm_out, hidden = model(inputs,hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        probs = probs.cpu()\n",
    "        \n",
    "    probs,index_positions = probs.topk(k)\n",
    "    \n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "    \n",
    "    probs = probs.numpy().flatten()\n",
    "    \n",
    "    probs = probs/probs.sum()\n",
    "    \n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "    return model.decoder[char],hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94943f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,size,seed='El',k=1):\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model,char, hidden, k=k)\n",
    "        \n",
    "    output_chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char,hidden = predict_next_char(model,output_chars[-1],hidden,k=k)\n",
    "        \n",
    "        output_chars.append(char)\n",
    "        \n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2304d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El de la contención de sus contradictoses de la casi de su presenteridad de la contradictiva, y se esta de los contrasiciones de los casas, ya el armo del alma de lo conciente de los complestes de la concensión, y en la acción de la completidad de la vocación del alma y de los contrados de sus concensibaciandoses de los casos, y se encaciertan al campo de los casos que se desenvolvan en la contradictiva, el armanio de los más contenes, y en lo en los casos del continio y de la caracte de lo de la vida, el cantario de sus complestas de las artes del continio de los mistos, en la viente de su propesiones de su alma y de los casas que sueño de su alte de los contentes del campio del preciso, en el alma de la contradictiva de sus propias del completo de su alma de la vida, en la vida de la concensión del alma de sustituidos dispositiones del arte de la contradicción de la contra la voluntad de la concertante, ella en los artes de las contentos y los comos de la vida, ya en el arte el candoro d\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,1000,seed='El ', k=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (CudaTorch)",
   "language": "python",
   "name": "cudatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
